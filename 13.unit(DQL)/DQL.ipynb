{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Derin Pekiştirmeli Ogrenme / Deep q Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install gym\n",
    "# %pip install pygame\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import random\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mrtmm\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      " 50%|█████     | 1/2 [00:52<00:52, 52.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0, time : 38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [02:17<00:00, 68.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1, time : 12\n",
      "time : 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time : 2\n",
      "time : 3\n",
      "time : 4\n",
      "time : 5\n",
      "time : 6\n",
      "time : 7\n",
      "time : 8\n",
      "time : 9\n",
      "time : 10\n",
      "time : 11\n",
      "time : 12\n",
      "time : 13\n",
      "time : 14\n",
      "time : 15\n",
      "time : 16\n",
      "time : 17\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "class DQLAgent:\n",
    "    def __init__(self, env):\n",
    "        #cevrenin gözlem alanı (state) boyutu\n",
    "        self.state_size = env.observation_space.shape[0]\n",
    "\n",
    "        #cevrede bulunan eylem sayisi (ajanın secebilcegi eylem sayisi)\n",
    "        self.action_size = env.action_space.n\n",
    "\n",
    "        #gelecekteki odullerin indirim orani\n",
    "        self.gamma = 0.95\n",
    "        \n",
    "        # ajanın öğrenme hızı\n",
    "        self.learning_rate = 0.001\n",
    "        \n",
    "        # keşfetme oranı (epsilon) epsilon = 1 olsun maxiumum keşif\n",
    "        self.epsilon = 1.0\n",
    "        \n",
    "        # epsilonun her iterasyonda azalma oranı (epsilon azalıkça daha fazla öğrenme, daha az keşif)\n",
    "        self.epsilon_decay = 0.995\n",
    "        \n",
    "        # minimum keşfetme oranı (epsilon 00,1' in altına inmez)\n",
    "        self.epsilon_min = 0.01\n",
    "\n",
    "        # ajanın deneyimleri = bellek = geçmiş adımları\n",
    "        self.memory = deque(maxlen=1000)\n",
    "        \n",
    "        # derin öğrenme modelini inşa et\n",
    "        self.model = self.build_model() # ANN\n",
    "\n",
    "    def build_model(self):\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Dense(48, input_dim = self.state_size, activation=\"relu\"))\n",
    "        model.add(Dense(24, activation=\"relu\"))\n",
    "        model.add(Dense(self.action_size, activation=\"linear\"))\n",
    "        model.compile(loss = \"mse\", optimizer = Adam(learning_rate = self.learning_rate))\n",
    "\n",
    "        return model\n",
    "\n",
    "    # ajanın deneyimlerini bellek veri yapısına kaydet \n",
    "    def remember(self,state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "\n",
    "    # ajanımız eylem seçebilecek    \n",
    "    def act(self,state):\n",
    "        # eğer rastgele üretilen sayı epsilondan küçükse rastgele eylem seçilir (keşif)\n",
    "        if random.uniform(0,1) <= self.epsilon:\n",
    "            return env.action_space.sample() # rastgle eylem seç\n",
    "        \n",
    "        act_values = self.model.predict(state, verbose=0)\n",
    "        return np.argmax(act_values[0])\n",
    "    \n",
    "    # deneyimleri tekrar oynatarak deep q ağı eğitilir\n",
    "    def replay(self,batch_size):\n",
    "        # bellekte yeterince deneyim yoksa geri oynatma yapılmaz\n",
    "        if len(self.memory) < batch_size:\n",
    "            return\n",
    "        \n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            if done : # eger done ise bitis durum var ise odulu dogrudan hedef olarak aliriz\n",
    "                target = reward \n",
    "            else:\n",
    "                target = reward + self.gamma*np.amax(self.model.predict(next_state, verbose=0)[0])\n",
    "\n",
    "            # modelin tahmin ettigi oduller   \n",
    "            train_target = self.model.predict(state, verbose=0)\n",
    "\n",
    "            #ajanın yaptıgı eyleme gore tahmin edilen odulu guncelle\n",
    "            train_target[0][action] = target\n",
    "\n",
    "            #modeli egit \n",
    "            self.model.fit(state, train_target, verbose =0)\n",
    "\n",
    "    # epsilonun zamanla azalması yani keşif sömürü dengesi\n",
    "    def adaptiveEGreedy(self):\n",
    "\n",
    "        self.epsilon > self.epsilon_min \n",
    "        self.epsilon = self.epsilon * self.epsilon_decay\n",
    "\n",
    "# %% env kullanarak dql ajanı baslatma\n",
    "env = gym.make(\"CartPole-v1\", render_mode = \"human\") # cartpole ortamı baslatma\n",
    "agent = DQLAgent(env)\n",
    "\n",
    "batch_size = 32 # egitim için minibatch boyutu\n",
    "episodes = 2 # epochs, simulasyonun oynatılacagı toplam bolum sayisi\n",
    "\n",
    "for e in tqdm(range(episodes)):\n",
    "\n",
    "    # ortamı sıfırla baslangıc durumunu al \n",
    "    state = env.reset()[0] # ortamı sıfırlamak \n",
    "    state = np.reshape(state, [ 1,4])\n",
    "    time = 0 # zamanı adimi baslat\n",
    "    \n",
    "    while True:\n",
    "        # ajan eylem secer \n",
    "        action = agent.act(state)\n",
    "\n",
    "        # ajanımız ortamda bu eylemi uygular ve bu eylem sonucunda next_state,reward , bitis bilgisi(done) alir\n",
    "        (next_state, reward, done , _ , _ ) = env.step(action)\n",
    "        next_state = np.reshape(state,[1,4])\n",
    "\n",
    "        #yapmıs oldugu bu adimi yani eylemi ve bu eylem sonucu env alinan bilgileri kaydeder \n",
    "        agent.remember(state,action,reward,next_state,done)\n",
    "\n",
    "        # mevct durumu gğnceller\n",
    "        state = next_state\n",
    "\n",
    "        # deneyimleerden yeniden oynatmayı baslatır reply() => tarning\n",
    "        agent.replay(batch_size)\n",
    "\n",
    "        # epsilonu set eder \n",
    "        agent.adaptiveEGreedy()\n",
    "\n",
    "        # zaman adimini arttırır\n",
    "        time = time + 1\n",
    "\n",
    "        # eger done ise donguyu kirar ve bolum biter ve yeni bolume baslar \n",
    "        if done: \n",
    "            print(f\"Episode: {e}, time : {time}\")\n",
    "            break\n",
    "\n",
    "# test edilmesi \n",
    "import time \n",
    "\n",
    "trained_model = agent \n",
    "env = gym.make(\"CartPole-v1\", render_mode = \"human\") # cartpole ortamı baslatma\n",
    "\n",
    "state = env.reset()[0]\n",
    "state = np.reshape(state, [1,4])\n",
    "\n",
    "time_t = 0 \n",
    "\n",
    "while True:\n",
    "    env.render()\n",
    "    action = trained_model.act(state)\n",
    "    (next_state, reward, done , _ , _ ) = env.step(action)\n",
    "    next_state = np.reshape(next_state, [1,4])\n",
    "    state = next_state \n",
    "    time_t += 1 \n",
    "    print(f\"time : {time_t}\")\n",
    "\n",
    "    time.sleep(0.5)\n",
    "\n",
    "    if done:\n",
    "        break \n",
    "print (\"Done\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
